{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1CTDs_0B2SU7T3PNTJkDohsthu79Vxk70","authorship_tag":"ABX9TyO7vJF4dXEk1VdhXwe8txt4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"SVRP3gxJZKaW","executionInfo":{"status":"ok","timestamp":1721835599734,"user_tz":-330,"elapsed":88616,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"9f33f020-b3a9-444c-a22b-340a3f42084d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.64-py3-none-any.whl (824 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m824.8/824.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.64 ultralytics-thop-2.0.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kKC2xLRV3QN","executionInfo":{"status":"ok","timestamp":1721835077190,"user_tz":-330,"elapsed":423,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"f570ee00-9cc3-4a33-9f6f-df3c36751dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["720 1280 3\n"]}],"source":["import cv2\n","\n","video_path =\"/content/drive/MyDrive/Work_space/Project/sign_language/python_file/predict/Learn ASL Alphabet Video.mp4\"\n","\n","cap = cv2.VideoCapture(video_path)\n","ret, frame = cap.read()\n","#print(ret)\n","#print(frame)\n","H, W, _ = frame.shape\n","print(H, W,_)"]},{"cell_type":"code","source":["import cv2\n","\n","# Path to the video file\n","#video_path = 'path_to_your_video.mp4'\n","\n","# Create a VideoCapture object\n","cap = cv2.VideoCapture(video_path)\n","\n","# Check if the video was opened successfully\n","if not cap.isOpened():\n","    print(\"Error: Could not open video.\")\n","else:\n","    # Get the frames per second (FPS) of the video\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","\n","    # Calculate the number of frames per minute\n","    frames_per_minute = fps * 60\n","\n","    print(f\"Frames per second (FPS): {fps}\")\n","    print(f\"Frames per minute: {frames_per_minute}\")\n","\n","# Release the VideoCapture object\n","cap.release()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dv7fzCBMYBn7","executionInfo":{"status":"ok","timestamp":1721835208454,"user_tz":-330,"elapsed":402,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"d8565a68-0435-4514-b259-936165f445e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Frames per second (FPS): 29.97002997002997\n","Frames per minute: 1798.2017982017983\n"]}]},{"cell_type":"code","source":["import cv2\n","\n","# Path to the video file\n","#video_path = 'path_to_your_video.mp4'\n","\n","# Output directory to save selected frames\n","output_dir = '/output/'\n","\n","# Create a VideoCapture object\n","cap = cv2.VideoCapture(video_path)\n","\n","# Check if the video was opened successfully\n","if not cap.isOpened():\n","    print(\"Error: Could not open video.\")\n","else:\n","    # Get the frames per second (FPS) of the video\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    print(f\"Frames per second (FPS): {fps}\")\n","\n","    frame_count = 0\n","    selected_frame_count = 0\n","\n","    while True:\n","        # Read the next frame\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","            break\n","\n","        # Check if the current frame is the one we want to select\n","        if frame_count % int(fps) == 0:\n","            # Save the selected frame as an image file\n","            frame_filename = f\"{output_dir}frame_{selected_frame_count}.jpg\"\n","            cv2.imwrite(frame_filename, frame)\n","            selected_frame_count += 1\n","\n","        frame_count += 1\n","\n","# Release the VideoCapture object\n","cap.release()\n","\n","print(f\"Selected {selected_frame_count} frames from the video.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"up65CZKhYlbJ","executionInfo":{"status":"ok","timestamp":1721835804369,"user_tz":-330,"elapsed":8945,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"6fcfa9ef-e40a-4c77-8089-75528f123c14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Frames per second (FPS): 29.97002997002997\n","Selected 104 frames from the video.\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","\n","# Load a model\n","#model = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n","#model = YOLO(\"yolov8n.pt\")\n","model_path=\"/content/drive/MyDrive/Work_space/Project/sign_language/python_file/best.pt\"\n","\n","# Load a model\n","model = YOLO(model_path)  # load a custom model\n","\n","# Run batched inference on a list of images\n","results = model(\"/content/outputframe_60.jpg\", stream=True)  # return a generator of Results objects\n","#print(result)\n","\n","# Process results generator\n","for result in results:\n","    predict_out = result.boxes.data.tolist()\n","    print(predict_out) # Boxes object for bounding box outputs\n","    result.show()  # display to screen\n","    result.save(filename=\"result.jpg\")  # save to disk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOesXbC1ZI7v","executionInfo":{"status":"ok","timestamp":1721836155760,"user_tz":-330,"elapsed":2073,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"0189e565-7fa5-4029-ed3c-c91128c60240"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/outputframe_60.jpg: 384x640 (no detections), 191.0ms\n","[]\n","Speed: 4.8ms preprocess, 191.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"]}]},{"cell_type":"code","source":["import os\n","from ultralytics import YOLO\n","\n","# Load a model\n","#model = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n","#model = YOLO(\"yolov8n.pt\")\n","model_path=\"/content/drive/MyDrive/Work_space/Project/sign_language/python_file/best.pt\"\n","\n","# Load a model\n","model = YOLO(model_path)  # load a custom model\n","\n","# Path to the folder\n","folder_path = '/content/output'\n","\n","all_files = os.listdir(folder_path )\n","print(all_files)\n","for file in all_files:\n","  image_path=os.path.join(folder_path,file)\n","  # Run batched inference on a list of images\n","  results = model(image_path, stream=True)  # return a generator of Results objects\n","  #print(result)\n","\n","  # Process results generator\n","  for result in results:\n","      predict_out = result.boxes.data.tolist()\n","      print(predict_out) # Boxes object for bounding box outputs\n","      result.show()  # display to screen\n","      result.save(filename=\"result.jpg\")  # save to disk\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LUeQOpwhZ-nS","executionInfo":{"status":"ok","timestamp":1721836069212,"user_tz":-330,"elapsed":57313,"user":{"displayName":"Kaushi Gihan","userId":"11214181140146971518"}},"outputId":"df459c01-b2a0-4cc9-abfe-6e9bdeb2d7f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['frame_82.jpg', 'frame_32.jpg', 'frame_44.jpg', 'frame_11.jpg', 'frame_78.jpg', 'frame_42.jpg', 'frame_89.jpg', 'frame_98.jpg', 'frame_81.jpg', 'frame_23.jpg', 'frame_55.jpg', 'frame_72.jpg', 'frame_101.jpg', 'frame_40.jpg', 'frame_24.jpg', 'frame_94.jpg', 'frame_46.jpg', 'frame_39.jpg', 'frame_16.jpg', 'frame_14.jpg', 'frame_7.jpg', 'frame_51.jpg', 'frame_31.jpg', 'frame_70.jpg', 'frame_5.jpg', 'frame_52.jpg', 'frame_75.jpg', 'frame_99.jpg', 'frame_93.jpg', 'frame_21.jpg', 'frame_86.jpg', 'frame_80.jpg', 'frame_62.jpg', 'frame_64.jpg', 'frame_57.jpg', 'frame_33.jpg', 'frame_92.jpg', 'frame_45.jpg', 'frame_48.jpg', 'frame_2.jpg', 'frame_30.jpg', 'frame_54.jpg', 'frame_6.jpg', 'frame_74.jpg', 'frame_76.jpg', 'frame_53.jpg', 'frame_84.jpg', 'frame_97.jpg', 'frame_4.jpg', 'frame_68.jpg', 'frame_71.jpg', 'frame_61.jpg', 'frame_20.jpg', 'frame_77.jpg', 'frame_8.jpg', 'frame_17.jpg', 'frame_35.jpg', 'frame_22.jpg', 'frame_26.jpg', 'frame_41.jpg', 'frame_25.jpg', 'frame_95.jpg', 'frame_43.jpg', 'frame_18.jpg', 'frame_73.jpg', 'frame_29.jpg', 'frame_47.jpg', 'frame_87.jpg', 'frame_19.jpg', 'frame_38.jpg', 'frame_59.jpg', 'frame_103.jpg', 'frame_12.jpg', 'frame_65.jpg', 'frame_83.jpg', 'frame_85.jpg', 'frame_91.jpg', 'frame_15.jpg', 'frame_56.jpg', 'frame_50.jpg', 'frame_100.jpg', 'frame_34.jpg', 'frame_27.jpg', 'frame_58.jpg', 'frame_49.jpg', 'frame_36.jpg', 'frame_9.jpg', 'frame_67.jpg', 'frame_96.jpg', 'frame_88.jpg', 'frame_90.jpg', 'frame_28.jpg', 'frame_10.jpg', 'frame_102.jpg', 'frame_66.jpg', 'frame_60.jpg', 'frame_3.jpg', 'frame_1.jpg', 'frame_63.jpg', 'frame_0.jpg', 'frame_37.jpg', 'frame_13.jpg', 'frame_79.jpg', 'frame_69.jpg']\n","\n","image 1/1 /content/output/frame_82.jpg: 384x640 (no detections), 352.9ms\n","[]\n","Speed: 40.7ms preprocess, 352.9ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_32.jpg: 384x640 (no detections), 185.4ms\n","[]\n","Speed: 2.7ms preprocess, 185.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_44.jpg: 384x640 (no detections), 150.4ms\n","[]\n","Speed: 2.5ms preprocess, 150.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_11.jpg: 384x640 (no detections), 151.1ms\n","[]\n","Speed: 2.3ms preprocess, 151.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_78.jpg: 384x640 1 I Love You, 155.0ms\n","[[541.2677001953125, 168.3062744140625, 766.655517578125, 559.18359375, 0.6701178550720215, 23.0]]\n","Speed: 2.5ms preprocess, 155.0ms inference, 12.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_42.jpg: 384x640 (no detections), 194.5ms\n","[]\n","Speed: 2.3ms preprocess, 194.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_89.jpg: 384x640 1 I Love You, 156.7ms\n","[[854.017822265625, 291.57952880859375, 960.981201171875, 469.77679443359375, 0.9809498190879822, 23.0]]\n","Speed: 2.3ms preprocess, 156.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_98.jpg: 384x640 (no detections), 148.0ms\n","[]\n","Speed: 2.4ms preprocess, 148.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_81.jpg: 384x640 (no detections), 163.0ms\n","[]\n","Speed: 2.8ms preprocess, 163.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_23.jpg: 384x640 (no detections), 163.1ms\n","[]\n","Speed: 2.5ms preprocess, 163.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_55.jpg: 384x640 (no detections), 173.4ms\n","[]\n","Speed: 2.8ms preprocess, 173.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_72.jpg: 384x640 (no detections), 166.0ms\n","[]\n","Speed: 2.3ms preprocess, 166.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_101.jpg: 384x640 1 Hello, 187.2ms\n","[[244.2344512939453, 201.007080078125, 494.74981689453125, 586.2940063476562, 0.6102307438850403, 19.0]]\n","Speed: 3.2ms preprocess, 187.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_40.jpg: 384x640 (no detections), 183.0ms\n","[]\n","Speed: 2.4ms preprocess, 183.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_24.jpg: 384x640 (no detections), 246.3ms\n","[]\n","Speed: 2.9ms preprocess, 246.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_94.jpg: 384x640 1 I Love You, 201.2ms\n","[[855.095947265625, 290.44598388671875, 953.164306640625, 470.24822998046875, 0.9457169771194458, 23.0]]\n","Speed: 4.7ms preprocess, 201.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_46.jpg: 384x640 (no detections), 255.1ms\n","[]\n","Speed: 2.8ms preprocess, 255.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_39.jpg: 384x640 (no detections), 231.1ms\n","[]\n","Speed: 2.6ms preprocess, 231.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_16.jpg: 384x640 (no detections), 261.3ms\n","[]\n","Speed: 10.9ms preprocess, 261.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_14.jpg: 384x640 (no detections), 236.4ms\n","[]\n","Speed: 2.7ms preprocess, 236.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_7.jpg: 384x640 (no detections), 232.9ms\n","[]\n","Speed: 4.9ms preprocess, 232.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_51.jpg: 384x640 (no detections), 253.9ms\n","[]\n","Speed: 2.5ms preprocess, 253.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_31.jpg: 384x640 (no detections), 252.1ms\n","[]\n","Speed: 5.9ms preprocess, 252.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_70.jpg: 384x640 (no detections), 265.9ms\n","[]\n","Speed: 2.7ms preprocess, 265.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_5.jpg: 384x640 (no detections), 223.4ms\n","[]\n","Speed: 5.7ms preprocess, 223.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_52.jpg: 384x640 (no detections), 166.4ms\n","[]\n","Speed: 2.3ms preprocess, 166.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_75.jpg: 384x640 1 I Love You, 157.0ms\n","[[549.32373046875, 188.09674072265625, 737.1004638671875, 573.620361328125, 0.5516244173049927, 23.0]]\n","Speed: 2.4ms preprocess, 157.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_99.jpg: 384x640 (no detections), 147.9ms\n","[]\n","Speed: 2.5ms preprocess, 147.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_93.jpg: 384x640 2 I Love Yous, 170.3ms\n","[[849.8599853515625, 292.0240783691406, 945.6156005859375, 459.3694763183594, 0.959386944770813, 23.0], [309.41363525390625, 295.13055419921875, 585.4180908203125, 547.2845458984375, 0.8038578033447266, 23.0]]\n","Speed: 2.9ms preprocess, 170.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_21.jpg: 384x640 (no detections), 161.5ms\n","[]\n","Speed: 2.3ms preprocess, 161.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_86.jpg: 384x640 (no detections), 166.6ms\n","[]\n","Speed: 3.1ms preprocess, 166.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_80.jpg: 384x640 1 I Love You, 171.5ms\n","[[576.6964111328125, 201.145263671875, 753.585205078125, 511.59332275390625, 0.27952608466148376, 23.0]]\n","Speed: 2.6ms preprocess, 171.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_62.jpg: 384x640 1 Internet, 301.5ms\n","[[600.20947265625, 293.88616943359375, 766.319580078125, 592.5955810546875, 0.3599165081977844, 24.0]]\n","Speed: 4.9ms preprocess, 301.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_64.jpg: 384x640 (no detections), 149.1ms\n","[]\n","Speed: 2.5ms preprocess, 149.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_57.jpg: 384x640 (no detections), 149.2ms\n","[]\n","Speed: 2.6ms preprocess, 149.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_33.jpg: 384x640 (no detections), 167.9ms\n","[]\n","Speed: 2.9ms preprocess, 167.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_92.jpg: 384x640 (no detections), 142.8ms\n","[]\n","Speed: 2.4ms preprocess, 142.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_45.jpg: 384x640 (no detections), 144.4ms\n","[]\n","Speed: 2.4ms preprocess, 144.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_48.jpg: 384x640 1 I Love You, 150.6ms\n","[[560.88427734375, 171.5357666015625, 854.851806640625, 553.0262451171875, 0.9039734601974487, 23.0]]\n","Speed: 2.3ms preprocess, 150.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_2.jpg: 384x640 1 I Love You, 160.5ms\n","[[525.555419921875, 185.7160186767578, 734.276611328125, 567.8168334960938, 0.820127546787262, 23.0]]\n","Speed: 2.3ms preprocess, 160.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_30.jpg: 384x640 (no detections), 176.6ms\n","[]\n","Speed: 2.7ms preprocess, 176.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_54.jpg: 384x640 (no detections), 147.1ms\n","[]\n","Speed: 2.4ms preprocess, 147.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_6.jpg: 384x640 (no detections), 160.0ms\n","[]\n","Speed: 2.3ms preprocess, 160.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_74.jpg: 384x640 1 I Love You, 153.7ms\n","[[573.917236328125, 189.98614501953125, 773.8514404296875, 551.7001342773438, 0.7840994000434875, 23.0]]\n","Speed: 2.6ms preprocess, 153.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_76.jpg: 384x640 1 I Love You, 145.0ms\n","[[555.3826904296875, 186.339111328125, 732.2218017578125, 577.4718627929688, 0.6295827031135559, 23.0]]\n","Speed: 2.3ms preprocess, 145.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_53.jpg: 384x640 (no detections), 157.2ms\n","[]\n","Speed: 2.2ms preprocess, 157.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_84.jpg: 384x640 (no detections), 153.0ms\n","[]\n","Speed: 2.3ms preprocess, 153.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_97.jpg: 384x640 (no detections), 141.6ms\n","[]\n","Speed: 2.4ms preprocess, 141.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_4.jpg: 384x640 (no detections), 168.3ms\n","[]\n","Speed: 2.7ms preprocess, 168.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_68.jpg: 384x640 (no detections), 149.1ms\n","[]\n","Speed: 3.1ms preprocess, 149.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_71.jpg: 384x640 (no detections), 268.1ms\n","[]\n","Speed: 2.6ms preprocess, 268.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_61.jpg: 384x640 1 Internet, 252.5ms\n","[[578.2459716796875, 290.6519775390625, 751.751953125, 594.07421875, 0.2760063409805298, 24.0]]\n","Speed: 2.9ms preprocess, 252.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_20.jpg: 384x640 (no detections), 251.0ms\n","[]\n","Speed: 5.4ms preprocess, 251.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_77.jpg: 384x640 1 I Love You, 225.3ms\n","[[536.347412109375, 168.4314727783203, 770.28076171875, 554.0512084960938, 0.7327052354812622, 23.0]]\n","Speed: 3.6ms preprocess, 225.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_8.jpg: 384x640 (no detections), 252.8ms\n","[]\n","Speed: 5.2ms preprocess, 252.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_17.jpg: 384x640 (no detections), 250.4ms\n","[]\n","Speed: 7.1ms preprocess, 250.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_35.jpg: 384x640 1 I Love You, 259.4ms\n","[[528.3765869140625, 171.81005859375, 780.1561279296875, 549.413330078125, 0.3279685974121094, 23.0]]\n","Speed: 2.7ms preprocess, 259.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_22.jpg: 384x640 (no detections), 255.7ms\n","[]\n","Speed: 5.0ms preprocess, 255.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_26.jpg: 384x640 1 Hello, 153.1ms\n","[[556.5263671875, 228.54164123535156, 796.3524169921875, 589.2393798828125, 0.32585689425468445, 19.0]]\n","Speed: 2.6ms preprocess, 153.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_41.jpg: 384x640 (no detections), 172.6ms\n","[]\n","Speed: 2.8ms preprocess, 172.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_25.jpg: 384x640 1 Hello, 169.0ms\n","[[557.5650634765625, 233.95208740234375, 808.7308349609375, 589.4878540039062, 0.47622424364089966, 19.0]]\n","Speed: 2.7ms preprocess, 169.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_95.jpg: 384x640 1 I Love You, 144.4ms\n","[[858.603515625, 333.6287841796875, 956.982177734375, 476.930908203125, 0.2548648715019226, 23.0]]\n","Speed: 2.3ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_43.jpg: 384x640 (no detections), 166.0ms\n","[]\n","Speed: 2.6ms preprocess, 166.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_18.jpg: 384x640 (no detections), 150.9ms\n","[]\n","Speed: 2.4ms preprocess, 150.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_73.jpg: 384x640 (no detections), 181.7ms\n","[]\n","Speed: 2.3ms preprocess, 181.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_29.jpg: 384x640 (no detections), 159.1ms\n","[]\n","Speed: 2.7ms preprocess, 159.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_47.jpg: 384x640 1 Hello, 188.5ms\n","[[560.1180419921875, 183.21986389160156, 719.9178466796875, 591.1099853515625, 0.4922160506248474, 19.0]]\n","Speed: 2.7ms preprocess, 188.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_87.jpg: 384x640 2 I Love Yous, 169.3ms\n","[[849.661865234375, 293.10797119140625, 969.074951171875, 464.91790771484375, 0.990936815738678, 23.0], [359.0419921875, 210.37799072265625, 552.718017578125, 534.3011474609375, 0.36560487747192383, 23.0]]\n","Speed: 2.4ms preprocess, 169.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_19.jpg: 384x640 (no detections), 143.0ms\n","[]\n","Speed: 2.3ms preprocess, 143.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_38.jpg: 384x640 (no detections), 176.4ms\n","[]\n","Speed: 2.4ms preprocess, 176.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_59.jpg: 384x640 1 Internet, 156.2ms\n","[[551.5235595703125, 294.50994873046875, 768.7930908203125, 593.43505859375, 0.29282522201538086, 24.0]]\n","Speed: 2.9ms preprocess, 156.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_103.jpg: 384x640 (no detections), 145.3ms\n","[]\n","Speed: 2.3ms preprocess, 145.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_12.jpg: 384x640 (no detections), 146.8ms\n","[]\n","Speed: 2.3ms preprocess, 146.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_65.jpg: 384x640 (no detections), 147.7ms\n","[]\n","Speed: 2.3ms preprocess, 147.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_83.jpg: 384x640 (no detections), 174.2ms\n","[]\n","Speed: 3.4ms preprocess, 174.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_85.jpg: 384x640 (no detections), 173.2ms\n","[]\n","Speed: 3.0ms preprocess, 173.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_91.jpg: 384x640 1 I Love You, 158.1ms\n","[[849.5977783203125, 289.41595458984375, 942.1378173828125, 466.41131591796875, 0.3724753260612488, 23.0]]\n","Speed: 2.8ms preprocess, 158.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_15.jpg: 384x640 1 Help, 153.5ms\n","[[566.4111328125, 204.6790771484375, 769.6912841796875, 521.1226196289062, 0.7070433497428894, 20.0]]\n","Speed: 2.5ms preprocess, 153.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_56.jpg: 384x640 (no detections), 158.4ms\n","[]\n","Speed: 2.6ms preprocess, 158.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_50.jpg: 384x640 1 I Love You, 157.8ms\n","[[534.7843017578125, 170.493408203125, 803.9342041015625, 544.9697265625, 0.6673892736434937, 23.0]]\n","Speed: 2.4ms preprocess, 157.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_100.jpg: 384x640 (no detections), 151.0ms\n","[]\n","Speed: 2.7ms preprocess, 151.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_34.jpg: 384x640 2 I Love Yous, 146.2ms\n","[[515.865966796875, 167.22021484375, 762.48876953125, 524.7084350585938, 0.49483200907707214, 23.0], [529.8824462890625, 167.15992736816406, 713.8402099609375, 489.8485107421875, 0.39144274592399597, 23.0]]\n","Speed: 2.4ms preprocess, 146.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_27.jpg: 384x640 1 Hello, 171.2ms\n","[[546.2296142578125, 247.54833984375, 799.0460205078125, 592.145751953125, 0.28399351239204407, 19.0]]\n","Speed: 2.5ms preprocess, 171.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_58.jpg: 384x640 1 I Love You, 234.6ms\n","[[541.6395263671875, 119.0928955078125, 762.0174560546875, 474.87396240234375, 0.5400003790855408, 23.0]]\n","Speed: 3.6ms preprocess, 234.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_49.jpg: 384x640 1 I Love You, 236.2ms\n","[[533.85302734375, 171.0779266357422, 823.13134765625, 555.0404663085938, 0.9027460813522339, 23.0]]\n","Speed: 5.0ms preprocess, 236.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_36.jpg: 384x640 (no detections), 272.0ms\n","[]\n","Speed: 4.9ms preprocess, 272.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_9.jpg: 384x640 (no detections), 226.2ms\n","[]\n","Speed: 2.7ms preprocess, 226.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_67.jpg: 384x640 (no detections), 230.1ms\n","[]\n","Speed: 5.4ms preprocess, 230.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_96.jpg: 384x640 (no detections), 259.3ms\n","[]\n","Speed: 3.2ms preprocess, 259.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_88.jpg: 384x640 1 I Love You, 254.1ms\n","[[851.2318725585938, 293.697265625, 960.0208129882812, 466.1077880859375, 0.9725807309150696, 23.0]]\n","Speed: 2.6ms preprocess, 254.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_90.jpg: 384x640 1 I Love You, 233.5ms\n","[[851.0521850585938, 289.41571044921875, 958.8322143554688, 468.95660400390625, 0.9413379430770874, 23.0]]\n","Speed: 2.7ms preprocess, 233.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_28.jpg: 384x640 1 Hello, 155.2ms\n","[[532.1640625, 225.21270751953125, 711.605224609375, 592.2293701171875, 0.26652437448501587, 19.0]]\n","Speed: 4.2ms preprocess, 155.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_10.jpg: 384x640 (no detections), 149.9ms\n","[]\n","Speed: 2.4ms preprocess, 149.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_102.jpg: 384x640 (no detections), 156.6ms\n","[]\n","Speed: 2.3ms preprocess, 156.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_66.jpg: 384x640 (no detections), 151.2ms\n","[]\n","Speed: 2.3ms preprocess, 151.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_60.jpg: 384x640 (no detections), 145.8ms\n","[]\n","Speed: 2.3ms preprocess, 145.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_3.jpg: 384x640 1 I Love You, 179.0ms\n","[[532.1373291015625, 162.68104553222656, 750.5999755859375, 564.376708984375, 0.5178878307342529, 23.0]]\n","Speed: 2.6ms preprocess, 179.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_1.jpg: 384x640 1 I Love You, 149.4ms\n","[[503.469970703125, 220.14268493652344, 765.7950439453125, 589.30126953125, 0.5890752077102661, 23.0]]\n","Speed: 2.6ms preprocess, 149.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_63.jpg: 384x640 (no detections), 147.2ms\n","[]\n","Speed: 2.6ms preprocess, 147.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_0.jpg: 384x640 (no detections), 150.1ms\n","[]\n","Speed: 2.5ms preprocess, 150.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_37.jpg: 384x640 (no detections), 151.5ms\n","[]\n","Speed: 2.3ms preprocess, 151.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_13.jpg: 384x640 (no detections), 155.7ms\n","[]\n","Speed: 2.3ms preprocess, 155.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_79.jpg: 384x640 (no detections), 143.1ms\n","[]\n","Speed: 2.3ms preprocess, 143.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/output/frame_69.jpg: 384x640 (no detections), 142.0ms\n","[]\n","Speed: 2.3ms preprocess, 142.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"]}]}]}